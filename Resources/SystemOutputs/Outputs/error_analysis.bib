@InProceedings{papineni-EtAl:2002:ACL,
  author    = {Kishore Papineni  and  Salim Roukos  and  Todd Ward  and  Wei-Jing Zhu},
  title     = {Bleu: a Method for Automatic Evaluation of Machine Translation},
  booktitle = {Proceedings of 40th Annual Meeting of the Association for Computational Linguistics},
  month     = {July},
  year      = {2002},
  address   = {Philadelphia, Pennsylvania, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {311--318},
  url       = {http://www.aclweb.org/anthology/P02-1040},
  doi       = {10.3115/1073083.1073135}
}


@InProceedings{denkowski-lavie:2014:W14-33,
  author    = {Denkowski, Michael  and  Lavie, Alon},
  title     = {Meteor Universal: Language Specific Translation Evaluation for Any Target Language},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {376--380},
  url       = {http://www.aclweb.org/anthology/W14-3348}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}




@inproceedings{Lin:2003:AES:1073445.1073465,
 author = {Lin, Chin-Yew and Hovy, Eduard},
 title = {Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics},
 booktitle = {Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1},
 series = {NAACL '03},
 year = {2003},
 location = {Edmonton, Canada},
 pages = {71--78},
 numpages = {8},
 url = {https://doi.org/10.3115/1073445.1073465},
 doi = {10.3115/1073445.1073465},
 acmid = {1073465},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

@article{bernardi2016automatic,
  title={Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures.},
  author={Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
  journal={Journal of Artificial Intelligence Research},
  volume={55},
  pages={409--442},
  year={2016}
}

@inproceedings{AAAI1714249,
	author = {Minghai Chen and Guiguang Ding and Sicheng Zhao and Hui Chen and Qiang Liu and Jungong Han},
	title = {Reference Based LSTM for Image Captioning},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2017},
	keywords = {},
	url = {https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14249}
}

@InProceedings{Chen_2017_CVPR,
author = {Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
title = {SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{liu2017attention,
  title={Attention Correctness in Neural Image Captioning.},
  author={Liu, Chenxi and Mao, Junhua and Sha, Fei and Yuille, Alan L},
  booktitle={AAAI},
  pages={4176--4182},
  year={2017}
}


@InProceedings{Gu_2017_ICCV,
author = {Gu, Jiuxiang and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
title = {An Empirical Study of Language CNN for Image Captioning},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@inproceedings{Wang:2016:DIC:3060832.3061035,
 author = {Wang, Zhuhao and Wu, Fei and Lu, Weiming and Xiao, Jun and Li, Xi and Zhang, Zitong and Zhuang, Yueting},
 title = {Diverse Image Captioning via Grouptalk},
 booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
 series = {IJCAI'16},
 year = {2016},
 isbn = {978-1-57735-770-4},
 location = {New York, New York, USA},
 pages = {2957--2964},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3060832.3061035},
 acmid = {3061035},
 publisher = {AAAI Press},
}


@inproceedings{ranzato2016sequence,
	Author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
	Booktitle = {Proceedings of the International Conference on Learning Representations},
	Date-Added = {2017-10-30 18:50:59 +0000},
	Date-Modified = {2017-10-30 18:51:50 +0000},
	Title = {Sequence level training with recurrent neural networks},
	Year = {2016}}


@inproceedings{Shetty:2016:ESC:2983563.2983571,
 author = {Shetty, Rakshith and R.-Tavakoli, Hamed and Laaksonen, Jorma},
 title = {Exploiting Scene Context for Image Captioning},
 booktitle = {Proceedings of the 2016 ACM Workshop on Vision and Language Integration Meets Multimedia Fusion},
 series = {iV\&L-MM '16},
 year = {2016},
 isbn = {978-1-4503-4519-4},
 location = {Amsterdam, The Netherlands},
 pages = {1--8},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2983563.2983571},
 doi = {10.1145/2983563.2983571},
 acmid = {2983571},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer vision, deep learning, image captioning, long-short term memory},
}


@article{Chang2017,
	Abstract = {Despite the progress, generating natural language descriptions for images is still a challenging task. Most state-of-the-art methods for solving this problem apply existing deep convolutional neural network (CNN) models to extract a visual representation of the entire image, based on which the parallel structures between images and sentences are exploited using recurrent neural networks. However, there is an inherent drawback that their models may attend to a partial view of a visual element or a conglomeration of several concepts. In this paper, we present a fine-grained attention based model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation. The model contains three sub-networks: a deep recurrent neural network for sentences, a deep convolutional network for images, and a region proposal network for nearly cost-free region proposals. Our model is able to automatically learn to fix its gaze on salient region proposals. The process of generating the next word, given the previously generated ones, is aligned with this visual perception experience. We validate the effectiveness of the proposed model on three benchmark datasets (Flickr 8K, Flickr 30K and MS COCO). The experimental results confirm the effectiveness of the proposed system.},
	Author = {Chang, Yan-Shuo},
	Day = {30},
	Doi = {10.1007/s11042-017-4593-1},
	Issn = {1573-7721},
	Journal = {Multimedia Tools and Applications},
	Month = {Mar},
	Title = {Fine-grained attention for image caption generation},
	Url = {https://doi.org/10.1007/s11042-017-4593-1},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11042-017-4593-1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s11042-017-4593-1}}

@InProceedings{Gan_2017_CVPR,
author = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
title = {Semantic Compositional Networks for Visual Captioning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{li2017image,
  title={Image Caption with Global-Local Attention.},
  author={Li, Linghui and Tang, Sheng and Deng, Lixi and Zhang, Yongdong and Tian, Qi},
  booktitle={AAAI},
  pages={4133--4139},
  year={2017}
}

@inproceedings{Li:2016:ICB:2964284.2984069,
 author = {Li, Xiangyang and Song, Xinhang and Herranz, Luis and Zhu, Yaohui and Jiang, Shuqiang},
 title = {Image Captioning with Both Object and Scene Information},
 booktitle = {Proceedings of the 2016 ACM on Multimedia Conference},
 series = {MM '16},
 year = {2016},
 isbn = {978-1-4503-3603-1},
 location = {Amsterdam, The Netherlands},
 pages = {1107--1110},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2964284.2984069},
 doi = {10.1145/2964284.2984069},
 acmid = {2984069},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {convolutional neural network, image caption, long short-term memory, scene and object},
}


@article{wu2017image,
  title={Image Captioning and Visual Question Answering Based on Attributes and External Knowledge},
  author={Wu, Qi and Shen, Chunhua and Wang, Peng and Dick, Anthony and van den Hengel, Anton},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  publisher={IEEE}
}

@ARTICLE{8031355,
author={Linghui Li and Sheng Tang and Yongdong Zhang and Lixi Deng and Qi Tian},
journal={IEEE Transactions on Multimedia},
title={GLA: Global-local Attention for Image Description},
year={2017},
volume={PP},
number={99},
pages={1-1},
keywords={Computational modeling;Decoding;Feature extraction;Image recognition;Natural language processing;Recurrent neural networks;Convolutional Neural Network;Natural Language Processing;Recurrent Neural Network;image description},
doi={10.1109/TMM.2017.2751140},
ISSN={1520-9210},
month={},}

@inproceedings{tavakoli2017paying,
  title={Paying Attention to Descriptions Generated by Image Captioning Models},
  author={Tavakoli, Hamed R and Shetty, Rakshith and Borji, Ali and Laaksonen, Jorma},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2487--2496},
  year={2017}
}

        @inproceedings{BMVC2016_141,
        	title={Oracle Performance for Visual Captioning},
        	author={Li Yao and Nicolas Ballas and Kyunghyun Cho and John Smith and Yoshua Bengio},
        	year={2016},
        	month={September},
        	pages={141.1-141.13},
        	articleno={141},
        	numpages={13},
        	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
        	publisher={BMVA Press},
        	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},
        	doi={10.5244/C.30.141},
        	isbn={1-901725-59-6},
        	url={https://dx.doi.org/10.5244/C.30.141}
        }


@inproceedings{zhou2017watch,
	Author = {Zhou, Luowei and Xu, Chenliang and Koch, Parker and Corso, Jason J},
	Booktitle = {Proceedings of the ACM International Conference on Multimedia Thematic Workshops},
	Date-Added = {2017-10-30 18:42:41 +0000},
	Date-Modified = {2017-10-30 18:43:32 +0000},
	Title = {Watch What You Just Said: Image Captioning with Text-Conditional Attention},
	Year = {2017}}

@InProceedings{fokkens-EtAl:2013:ACL2013,
  author    = {Fokkens, Antske  and  van Erp, Marieke  and  Postma, Marten  and  Pedersen, Ted  and  Vossen, Piek  and  Freire, Nuno},
  title     = {Offspring from Reproduction Problems: What Replication Failure Teaches Us},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2013},
  address   = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics},
  pages     = {1691--1701},
  url       = {http://www.aclweb.org/anthology/P13-1166}
}

@incollection{NIPS2016_6167,
	Author = {Yang, Zhilin and Yuan, Ye and Wu, Yuexin and Cohen, William W and Salakhutdinov, Ruslan R},
	Booktitle = {Advances in Neural Information Processing Systems 29},
	Editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	Pages = {2361--2369},
	Publisher = {Curran Associates, Inc.},
	Title = {Review Networks for Caption Generation},
	Url = {http://papers.nips.cc/paper/6167-review-networks-for-caption-generation.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6167-review-networks-for-caption-generation.pdf}}


@INPROCEEDINGS{7780398,
author={Qi Wu and Chunhua Shen and Lingqiao Liu and Anthony Dick and Anton van den Hengel},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={What Value Do Explicit High Level Concepts Have in Vision to Language Problems?},
year={2016},
volume={},
number={},
pages={203-212},
keywords={computer vision;question answering (information retrieval);recurrent neural nets;CNN;RNN;VQA;convolutional neural networks;high-level semantic concepts;image captioning;recurrent neural networks;vision-to-language problems;visual question answering;Computer vision;Feature extraction;Knowledge discovery;Semantics;Training;Visualization;Vocabulary},
doi={10.1109/CVPR.2016.29},
ISSN={},
month={June},}

@InProceedings{shekhar-EtAl:2017:Long,
  author    = {Shekhar, Ravi  and  Pezzelle, Sandro  and  Klimovich, Yauhen  and  Herbelot, Aur\'{e}lie  and  Nabi, Moin  and  Sangineto, Enver  and  Bernardi, Raffaella},
  title     = {FOIL it! Find One mismatch between Image and Language caption},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {255--265},
  abstract  = {In this paper, we aim to understand whether current language and vision (LaVi)
	models truly grasp the interaction between the two modalities. To this end, we
	propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images
	with both correct and `foil' captions, that is, descriptions of the image that
	are highly similar to the original ones, but contain one single mistake (`foil
	word'). We show that current LaVi models fall into the traps of this data and
	perform badly on three tasks: a) caption  classification (correct vs. foil); b)
	foil word detection; c) foil word correction. Humans, in contrast, have
	near-perfect performance on those tasks. We demonstrate that merely utilising
	language cues is not enough to model FOIL-COCO and that it challenges the
	state-of-the-art by requiring a fine-grained understanding of the relation
	between text and image.},
  url       = {http://aclweb.org/anthology/P17-1024}
}


@InProceedings{graham-EtAl:2016:COLING,
  author    = {Graham, Yvette  and  Baldwin, Timothy  and  Dowling, Meghan  and  Eskevich, Maria  and  Lynn, Teresa  and  Tounsi, Lamia},
  title     = {Is all that Glitters in Machine Translation Quality Estimation really Gold?},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  month     = {December},
  year      = {2016},
  address   = {Osaka, Japan},
  publisher = {The COLING 2016 Organizing Committee},
  pages     = {3124--3134},
  abstract  = {Human-targeted metrics provide a compromise between human evaluation of machine
	translation, where high inter-annotator agreement is difficult to achieve, and
	fully automatic metrics, such as BLEU or TER, that lack the validity of human
	assessment. Human-targeted translation edit rate (HTER) is by far the most
	widely employed human-targeted metric in machine translation, commonly
	employed, for example, as a gold standard in evaluation of quality estimation.
	Original experiments justifying the design of HTER, as opposed to other
	possible formulations, were limited to a small sample of translations and a
	single language pair, however, and this motivates our re-evaluation of a range
	of human-targeted metrics on a substantially larger scale. Results show
	significantly stronger correlation with human judgment for HBLEU over HTER for
	two of the nine language pairs we include and no significant difference between
	correlations
	achieved by HTER and HBLEU for the remaining language pairs. Finally, we
	evaluate a range of quality estimation systems employing HTER and direct
	assessment (DA) of translation adequacy as gold labels, resulting in a
	divergence in system rankings, and propose employment of DA for future quality
	estimation evaluations.},
  url       = {http://aclweb.org/anthology/C16-1294}
}

@InProceedings{graham-baldwin-mathur:2015:NAACL-HLT,
  author    = {Graham, Yvette  and  Baldwin, Timothy  and  Mathur, Nitika},
  title     = {Accurate Evaluation of Segment-level Machine Translation Metrics},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {May--June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {1183--1191},
  url       = {http://www.aclweb.org/anthology/N15-1124}
}


@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}



@InProceedings{hodosh-hockenmaier:2016:VL16,
  author    = {Hodosh, Micah  and  Hockenmaier, Julia},
  title     = {Focused Evaluation for Image Description with Binary Forced-Choice Tasks},
  booktitle = {Proceedings of the 5th Workshop on Vision and Language},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {19--28},
  url       = {http://anthology.aclweb.org/W16-3203}
}

@article{TACL229,
	author = {Young, Peter  and Lai, Alice  and Hodosh, Micah  and Hockenmaier, Julia },
	title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {2},
	year = {2014},
	keywords = {},
	abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.},
	issn = {2307-387X},
	url = {https://transacl.org/ojs/index.php/tacl/article/view/229},
	pages = {67--78}
}

@inproceedings{Anderson2016,
	Address = {Cham},
	Author = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
	Booktitle = {Computer Vision -- ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V},
	Editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	Pages = {382--398},
	Publisher = {Springer International Publishing},
	Title = {SPICE: Semantic Propositional Image Caption Evaluation},
	Year = {2016}}


@article{ling2017teaching,
  title={Teaching Machines to Describe Images via Natural Language Feedback},
  author={Ling, Huan and Fidler, Sanja},
  journal={arXiv preprint arXiv:1706.00130},
  year={2017}
}


@inproceedings{ijcai2017-563,
	Author = {Chang Liu and Fuchun Sun and Changhu Wang and Feng Wang and Alan Yuille},
	Booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, {IJCAI-17}},
	Doi = {10.24963/ijcai.2017/563},
	Pages = {4033--4039},
	Title = {MAT: A Multimodal Attentive Translator for Image Captioning},
	Url = {https://doi.org/10.24963/ijcai.2017/563},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2017/563},
	Bdsk-Url-2 = {http://dx.doi.org/10.24963/ijcai.2017/563}}


@article{Song2016,
	Abstract = {In this paper, we propose an approach for generating rich fine-grained textual descriptions of images. In particular, we use an LSTM-in-LSTM (long short-term memory) architecture, which consists of an inner LSTM and an outer LSTM. The inner LSTM effectively encodes the long-range implicit contextual interaction between visual cues (i.e., the spatiallyconcurrent visual objects), while the outer LSTM generally captures the explicit multi-modal relationship between sentences and images (i.e., the correspondence of sentences and images). This architecture is capable of producing a long description by predicting one word at every time step conditioned on the previously generated word, a hidden vector (via the outer LSTM), and a context vector of fine-grained visual cues (via the inner LSTM). Our model outperforms state-of-theart methods on several benchmark datasets (Flickr8k, Flickr30k, MSCOCO) when used to generate long rich fine-grained descriptions of given images in terms of four different metrics (BLEU, CIDEr, ROUGE-L, and METEOR).},
	Author = {Song, Jun and Tang, Siliang and Xiao, Jun and Wu, Fei and Zhang, Zhongfei (Mark)},
	Day = {01},
	Doi = {10.1007/s41095-016-0059-z},
	Issn = {2096-0662},
	Journal = {Computational Visual Media},
	Month = {Dec},
	Number = {4},
	Pages = {379--388},
	Title = {LSTM-in-LSTM for generating long descriptions of images},
	Url = {https://doi.org/10.1007/s41095-016-0059-z},
	Volume = {2},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1007/s41095-016-0059-z},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s41095-016-0059-z}}


@inproceedings{Lu_2017_CVPR,
	Author = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Month = {July},
	Title = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},
	Year = {2017}}


@article{jin2017aligning,
	Author = {Jin, Junqi and Fu, Kun and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
	Date-Added = {2017-10-30 18:33:51 +0000},
	Date-Modified = {2017-10-30 18:34:51 +0000},
	Journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
	Note = {Preprint},
	Title = {Aligning where to see and what to tell: image caption with region-based attention and scene factorization},
	Url = {doi:10.1109/TPAMI.2016.2642953},
	Year = {2017}}



@incollection{NIPS2016_6528,
	Author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Yuan, Xin and Li, Chunyuan and Stevens, Andrew and Carin, Lawrence},
	Booktitle = {Advances in Neural Information Processing Systems 29},
	Editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	Pages = {2352--2360},
	Publisher = {Curran Associates, Inc.},
	Title = {Variational Autoencoder for Deep Learning of Images, Labels and Captions},
	Url = {http://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf}}


@inproceedings{Liu_2017_ICCV,
	Author = {Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin},
	Booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	Month = {Oct},
	Title = {Improved Image Captioning via Policy Gradient Optimization of SPIDEr},
	Year = {2017}}

@InProceedings{Dai_2017_ICCV,
author = {Dai, Bo and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua},
title = {Towards Diverse and Natural Image Descriptions via a Conditional GAN},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@inproceedings{pedersoli2017areas,
	Author = {Pedersoli, Marco and Lucas, Thomas and Schmid, Cordelia and Verbeek, Jakob},
	Booktitle = {International Conference on Computer Vision},
	Date-Added = {2017-10-30 16:13:14 +0000},
	Date-Modified = {2017-10-30 16:15:04 +0000},
	Month = {October},
	Title = {Areas of Attention for Image Captioning},
	Year = {2017}}


@inproceedings{Yao_2017_ICCV,
	Author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
	Booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	Month = {Oct},
	Title = {Boosting Image Captioning With Attributes},
	Year = {2017}}

@inproceedings{AAAI1714888,
	author = {Jonghwan Mun and Minsu Cho and Bohyung Han},
	title = {Text-Guided Attention Model for Image Captioning},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2017},
	keywords = {Image Captioning; Attention Model},
	abstract = {Visual attention plays an important role to understand images and demonstrates its effectiveness in generating natural language descriptions of images. On the other hand, recent studies show that language associated with an image can steer visual attention in the scene during our cognitive process. Inspired by this, we introduce a text-guided attention model for image captioning, which learns to drive visual attention using associated captions. For this model, we propose an exemplar-based learning approach that retrieves from training data associated captions with each image, and use them to learn attention on visual features. Our attention model enables to describe a detailed state of scenes by distinguishing small or confusable objects effectively. We validate our model on MS-COCO Captioning benchmark and achieve the state-of-the-art performance in standard metrics.},

	url = {https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14888}
}

@inproceedings{Rennie_2017_CVPR,
	Author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Month = {July},
	Title = {Self-Critical Sequence Training for Image Captioning},
	Year = {2017}}


@inproceedings{Shetty_2017_ICCV,
	Author = {Shetty, Rakshith and Rohrbach, Marcus and Anne Hendricks, Lisa and Fritz, Mario and Schiele, Bernt},
	Booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	Month = {Oct},
	Title = {Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training},
	Year = {2017}}


@InProceedings{Ren_2017_CVPR,
author = {Ren, Zhou and Wang, Xiaoyu and Zhang, Ning and Lv, Xutao and Li, Li-Jia},
title = {Deep Reinforcement Learning-Based Image Captioning With Embedding Reward},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

 @InProceedings{Wang_2017_CVPR,
author = {Wang, Yufei and Lin, Zhe and Shen, Xiaohui and Cohen, Scott and Cottrell, Garrison W.},
title = {Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}


@INPROCEEDINGS{7789551,
author={K. Tran and X. He and L. Zhang and J. Sun},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
title={Rich Image Captioning in the Wild},
year={2016},
volume={},
number={},
pages={434-441},
keywords={data handling;image recognition;MS COCO;confidence model;deep vision model;entity recognition model;high quality caption;human judgments;image captioning;in-domain dataset;microsoft cognitive services;out-of-domain data handling;visual concepts;Data models;Face;Knowledge based systems;Neural networks;Semantics;Training;Visualization},
doi={10.1109/CVPRW.2016.61},
ISSN={},
month={June},}

@ARTICLE{7505636,
author={O. Vinyals and A. Toshev and S. Bengio and D. Erhan},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge},
year={2017},
volume={39},
number={4},
pages={652-663},
keywords={artificial intelligence;computer vision;language translation;natural language processing;optimisation;recurrent neural nets;artificial intelligence;computer vision;deep recurrent architecture;image captioning;likelihood maximization;machine translation;natural language processing;natural sentence generation;target description sentence;Computational modeling;Computer vision;Logic gates;Natural languages;Recurrent neural networks;Training;Visualization;Image captioning;language model;recurrent neural network;sequence-to-sequence},
doi={10.1109/TPAMI.2016.2587640},
ISSN={0162-8828},
month={April},}

@inproceedings{yang2016review,
  title={Review networks for caption generation},
  author={Yang, Zhilin and Yuan, Ye and Wu, Yuexin and Cohen, William W and Salakhutdinov, Ruslan R},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2361--2369},
  year={2016}
}


@inproceedings{you2016image,
  title={Image captioning with semantic attention},
  author={You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4651--4659},
  year={2016}
}

@InProceedings{D15-1022,
  author = 	"Ramisa, Arnau
		and Wang, Josiah
		and Lu, Ying
		and Dellandrea, Emmanuel
		and Moreno-Noguer, Francesc
		and Gaizauskas, Robert",
  title = 	"Combining Geometric, Textual and Visual Features for Predicting      Prepositions in Image Descriptions    ",
  booktitle = 	"Proceedings of the 2015 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2015",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"214--220",
  location = 	"Lisbon, Portugal",
  doi = 	"10.18653/v1/D15-1022",
  url = 	"http://aclanthology.coli.uni-saarland.de/pdf/D/D15/D15-1022.pdf"
}
